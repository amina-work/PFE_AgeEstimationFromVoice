{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import precision_score\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n",
    "#importing dataset\n",
    "file_paths = [\n",
    "    \"D:/AMINA/PFE24/datasets/commonvoice/cv-valid-train.csv\",\n",
    "    \"D:/AMINA/PFE24/datasets/commonvoice/cv-other-train.csv\",\n",
    "    \"D:/AMINA/PFE24/datasets/commonvoice/cv-invalid.csv\"\n",
    "]\n",
    "\n",
    "dfs_train = []\n",
    "for i in file_paths:\n",
    "    df_train = pd.read_csv(i)\n",
    "    dfs_train.append(df_train)\n",
    "\n",
    "df_train = pd.concat(dfs_train, ignore_index=True)\n",
    "\n",
    "file_paths2 = [\n",
    "    \"D:/AMINA/PFE24/datasets/commonvoice/cv-valid-dev.csv\",\n",
    "    \"D:/AMINA/PFE24/datasets/commonvoice/cv-other-dev.csv\",\n",
    "]\n",
    "dfs_val = []\n",
    "for i in file_paths2:\n",
    "    df_val = pd.read_csv(i)\n",
    "    dfs_val.append(df_val)\n",
    "df_val = pd.concat(dfs_val, ignore_index=True)\n",
    "\n",
    "file_paths3 = [\n",
    "    \"D:/AMINA/PFE24/datasets/commonvoice/cv-valid-test.csv\",\n",
    "    \"D:/AMINA/PFE24/datasets/commonvoice/cv-other-test.csv\",\n",
    "]\n",
    "dfs_test = []\n",
    "for i in file_paths3:\n",
    "    df_test = pd.read_csv(i)\n",
    "    dfs_test.append(df_test)\n",
    "\n",
    "df_test = pd.concat(dfs_test, ignore_index=True)\n",
    "\n",
    "#data cleaning\n",
    "df_train = df_train.dropna(subset=['age'])\n",
    "df_train = df_train[[\"filename\",\"age\",\"gender\",\"accent\"]]\n",
    "df_train[\"gender\"].fillna(\"other\", inplace=True)\n",
    "df_train = df_train.dropna(subset=['accent'])\n",
    "#same for validation set\n",
    "df_val = df_val.dropna(subset=['age'])\n",
    "df_val = df_val[[\"filename\",\"age\",\"gender\",\"accent\"]]\n",
    "df_val[\"gender\"].fillna(\"other\", inplace=True)\n",
    "df_val = df_val.dropna(subset=['accent'])\n",
    "#same for testing set\n",
    "df_test = df_test.dropna(subset=['age'])\n",
    "df_test = df_test[[\"filename\",\"age\",\"gender\",\"accent\"]]\n",
    "# Convert 'age' column to numerical\n",
    "cleanup_nums = {\"age\": {\"teens\":1.0,\"twenties\":2.0,\"thirties\":3.0,\"fourties\":4.0,\"fifties\":5.0,\"sixties\":6.0,\"seventies\":7.0,\"eighties\":8.0}}\n",
    "df_test = df_test.replace(cleanup_nums)\n",
    "df_train = df_train.replace(cleanup_nums)\n",
    "#Convert categorical variables to numerical using one-hot encoding\n",
    "df_train = pd.get_dummies(df_train, columns=['gender', 'accent'])\n",
    "df_test = pd.get_dummies(df_test, columns=['gender', 'accent'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "model = Sequential([#define  a linear stack of layers for the mode\n",
    "    Embedding(input_dim=len(df_train.columns)-1, output_dim=64, input_length=1),#map categorical variables to dense vectors.\n",
    "    LSTM(units=64, activation='tanh', dropout=0.2, recurrent_dropout=0.2),#work for sequential data\n",
    "    Dense(32, activation='relu'),#fully connected layer with 32 neurons and ReLU activation function\n",
    "    Dropout(0.5),#prevent overfitting by randomaly setting fraction of input units to 0 at each update\n",
    "    Dense(8, activation='softmax')  # Softmax activation for multi-class classification to output probabilities for each class\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Adam is an optimization algorithm that is an extension to stochastic gradient descent\n",
    "#Sparse categorical cross-entropy is commonly used in multi-class classification problems where the labels are integers\n",
    "\n",
    "# Define training and validation data\n",
    "X_train = df_train.drop(columns=['filename', 'age']).values\n",
    "y_train = df_train['age'].values - 1  # Convert labels to start from 0\n",
    "X_val = df_val.drop(columns=['filename', 'age']).values\n",
    "y_val = df_val['age'].values - 1  # Convert labels to start from 0\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))\n",
    "# An epoch is one complete pass through the entire training dataset.\n",
    "\n",
    "# Evaluate the model on test data\n",
    "X_test = df_test.drop(columns=['filename', 'age']).values\n",
    "y_test = df_test['age'].values - 1  # Convert labels to start from 0\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define preprocessing steps\n",
    "filename_preprocessor = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  \n",
    "    ('encoder', OneHotEncoder())  \n",
    "])\n",
    "\n",
    "text_preprocessor = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),  \n",
    "])\n",
    "\n",
    "# Combine preprocessing steps for all features\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('filename', filename_preprocessor, ['filename']),\n",
    "    ('text', text_preprocessor, 'text'),  \n",
    "    ('other', 'passthrough', df.select_dtypes(exclude=\"object\").columns)  \n",
    "])\n",
    "\n",
    "# Split data into X (predictors) and y (target)\n",
    "X = df.drop(columns=['age'])\n",
    "y = df['age']\n",
    "\n",
    "# Split text data\n",
    "text_data = X['text']\n",
    "\n",
    "# Tokenize text data\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "X_text = tokenizer.texts_to_sequences(text_data)\n",
    "X_text = pad_sequences(X_text, maxlen=100)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, X_text_train, X_text_test, y_train, y_test = train_test_split(X, X_text, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define input layers for numeric and text data\n",
    "numeric_input = Input(shape=(X_train.shape[1],))\n",
    "text_input = Input(shape=(X_text_train.shape[1],))  # Use the shape of preprocessed text data\n",
    "\n",
    "# Define embedding layer for categorical data (filename)\n",
    "filename_embedding = Embedding(input_dim=len(df['filename'].unique()), output_dim=64, input_length=1)(numeric_input)\n",
    "\n",
    "# LSTM layer for text data\n",
    "lstm_layer = LSTM(units=64, activation='tanh', dropout=0.2, recurrent_dropout=0.2)(text_input)\n",
    "\n",
    "# Concatenate numerical and text embeddings\n",
    "concatenated = concatenate([filename_embedding, lstm_layer])\n",
    "\n",
    "# Additional layers\n",
    "dense_layer = Dense(32, activation='relu')(concatenated)\n",
    "dropout_layer = Dropout(0.5)(dense_layer)\n",
    "\n",
    "# Output layer\n",
    "output_layer = Dense(8, activation='softmax')(dropout_layer)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=[numeric_input, text_input], outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train, X_text_train], y_train, epochs=10, batch_size=32, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate([X_test, X_text_test], y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
